{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 658267,
          "sourceType": "datasetVersion",
          "datasetId": 277323
        }
      ],
      "dockerImageVersionId": 30407,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Plant Village Disease Classification | Acc: c790ea",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tcchzau/plantdissese/blob/main/Plant_Village_Disease_Classification_%7C_Acc_c790ea.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'plantvillage-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F277323%2F658267%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240516%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240516T082825Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D90727e8b1359689d1d4beabfdcda9f67e18812b80622a821cb21c7dfbee201c5e1d6577d2c84ecb620ad91822389874436f0b47bbaf6cbf954ac7dd3feec55ef997cd5eb0f98367e0e13702e5a9eb96e80e77b738a4e87fb48181f730d714c960d4a7f3ebf941a2014e273bea367b16a6cfc6314974c18874e649a9306b9655d89f25120f02fb9690fb00f4da1a5433464ec7bd854349dc93b882215b02f3136c4e4b5cd9473b22e39174952c7709066a9aedb0fb4e891690f5c00304a86f2495398c79c3835887695f782ddf7985deb8f2c83c08232ed7e9d0a5589e6cdf3778ad602712315ad98ba3d24a292053661d4878fe98b3760ade11676c3c458ff93'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "jq10tRMWhHDm",
        "outputId": "9918dbb1-c965-4f4e-a265-e4ef6003ec76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading plantvillage-dataset, 2189386419 bytes compressed\n",
            "[==================================================] 2189386419 bytes downloaded\n",
            "Downloaded and uncompressed: plantvillage-dataset\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import needed modules"
      ],
      "metadata": {
        "id": "CKeVGxZ5GG6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-05-16T08:24:33.454922Z",
          "iopub.execute_input": "2024-05-16T08:24:33.455387Z",
          "iopub.status.idle": "2024-05-16T08:27:03.442474Z",
          "shell.execute_reply.started": "2024-05-16T08:24:33.455347Z",
          "shell.execute_reply": "2024-05-16T08:27:03.440528Z"
        },
        "trusted": true,
        "id": "FCnAalYfhHDq",
        "outputId": "67710c7a-f104-4438-d1e5-52f9b7fe58bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.63.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import system libs\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import pathlib\n",
        "import itertools\n",
        "\n",
        "# import data handling tools\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# import Deep learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Ignore Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print ('modules loaded')"
      ],
      "metadata": {
        "id": "CeMcAy_5GG6s",
        "outputId": "5ec00148-b577-48fb-af51-faabfe96730f",
        "execution": {
          "iopub.status.busy": "2024-05-16T08:27:15.307364Z",
          "iopub.execute_input": "2024-05-16T08:27:15.307842Z",
          "iopub.status.idle": "2024-05-16T08:27:24.303113Z",
          "shell.execute_reply.started": "2024-05-16T08:27:15.307795Z",
          "shell.execute_reply": "2024-05-16T08:27:24.30139Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "modules loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create needed functions"
      ],
      "metadata": {
        "id": "SA_gwvwnGG6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to Create Data Frame from Dataset"
      ],
      "metadata": {
        "id": "e4reLHLHabWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Function to create data frame**"
      ],
      "metadata": {
        "id": "JQdhl_CRGG6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data paths with labels\n",
        "def define_paths(data_dir):\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "\n",
        "    folds = os.listdir(data_dir)\n",
        "    for fold in folds:\n",
        "        foldpath = os.path.join(data_dir, fold)\n",
        "        filelist = os.listdir(foldpath)\n",
        "        for file in filelist:\n",
        "            fpath = os.path.join(foldpath, file)\n",
        "            filepaths.append(fpath)\n",
        "            labels.append(fold)\n",
        "\n",
        "    return filepaths, labels\n",
        "\n",
        "\n",
        "# Concatenate data paths with labels into one dataframe ( to later be fitted into the model )\n",
        "def define_df(files, classes):\n",
        "    Fseries = pd.Series(files, name= 'filepaths')\n",
        "    Lseries = pd.Series(classes, name='labels')\n",
        "    return pd.concat([Fseries, Lseries], axis= 1)\n",
        "\n",
        "# Split dataframe to train, valid, and test\n",
        "def split_data(data_dir):\n",
        "    # train dataframe\n",
        "    files, classes = define_paths(data_dir)\n",
        "    df = define_df(files, classes)\n",
        "    strat = df['labels']\n",
        "    train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123, stratify= strat)\n",
        "\n",
        "    # valid and test dataframe\n",
        "    strat = dummy_df['labels']\n",
        "    valid_df, test_df = train_test_split(dummy_df,  train_size= 0.5, shuffle= True, random_state= 123, stratify= strat)\n",
        "\n",
        "    return train_df, valid_df, test_df"
      ],
      "metadata": {
        "id": "g2nDmYaAabWE",
        "execution": {
          "iopub.status.busy": "2024-05-16T08:28:02.810227Z",
          "iopub.execute_input": "2024-05-16T08:28:02.810776Z",
          "iopub.status.idle": "2024-05-16T08:28:02.826124Z",
          "shell.execute_reply.started": "2024-05-16T08:28:02.810723Z",
          "shell.execute_reply": "2024-05-16T08:28:02.824706Z"
        },
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Function to generate images from dataframe"
      ],
      "metadata": {
        "id": "JZaHdeFxGG6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gens (train_df, valid_df, test_df, batch_size):\n",
        "    '''\n",
        "    This function takes train, validation, and test dataframe and fit them into image data generator, because model takes data from image data generator.\n",
        "    Image data generator converts images into tensors. '''\n",
        "\n",
        "\n",
        "    # define model parameters\n",
        "    img_size = (224, 224)\n",
        "    channels = 3 # either BGR or Grayscale\n",
        "    color = 'rgb'\n",
        "    img_shape = (img_size[0], img_size[1], channels)\n",
        "\n",
        "    # Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
        "    ts_length = len(test_df)\n",
        "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
        "    test_steps = ts_length // test_batch_size\n",
        "\n",
        "    # This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
        "    def scalar(img):\n",
        "        return img\n",
        "\n",
        "    tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True)\n",
        "    ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
        "\n",
        "    train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
        "\n",
        "    valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
        "\n",
        "    # Note: we will use custom test_batch_size, and make shuffle= false\n",
        "    test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                        color_mode= color, shuffle= False, batch_size= test_batch_size)\n",
        "\n",
        "    return train_gen, valid_gen, test_gen"
      ],
      "metadata": {
        "id": "iLL8hHQcGG6x",
        "execution": {
          "iopub.status.busy": "2023-03-05T14:02:45.057017Z",
          "iopub.execute_input": "2023-03-05T14:02:45.057803Z",
          "iopub.status.idle": "2023-03-05T14:02:45.075398Z",
          "shell.execute_reply.started": "2023-03-05T14:02:45.057743Z",
          "shell.execute_reply": "2023-03-05T14:02:45.073934Z"
        },
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Function to display data sample**"
      ],
      "metadata": {
        "id": "8ifXox4SGG6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_images(gen):\n",
        "    '''\n",
        "    This function take the data generator and show sample of the images\n",
        "    '''\n",
        "\n",
        "    # return classes , images to be displayed\n",
        "    g_dict = gen.class_indices        # defines dictionary {'class': index}\n",
        "    classes = list(g_dict.keys())     # defines list of dictionary's kays (classes), classes names : string\n",
        "    images, labels = next(gen)        # get a batch size samples from the generator\n",
        "\n",
        "    # calculate number of displayed samples\n",
        "    length = len(labels)        # length of batch size\n",
        "    sample = min(length, 25)    # check if sample less than 25 images\n",
        "\n",
        "    plt.figure(figsize= (20, 20))\n",
        "\n",
        "    for i in range(sample):\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        image = images[i] / 255       # scales data to range (0 - 255)\n",
        "        plt.imshow(image)\n",
        "        index = np.argmax(labels[i])  # get image index\n",
        "        class_name = classes[index]   # get class of image\n",
        "        plt.title(class_name, color= 'blue', fontsize= 12)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IAGbj3ZyGG6y",
        "execution": {
          "iopub.status.busy": "2023-03-05T14:02:45.077786Z",
          "iopub.execute_input": "2023-03-05T14:02:45.078357Z",
          "iopub.status.idle": "2023-03-05T14:02:45.094682Z",
          "shell.execute_reply.started": "2023-03-05T14:02:45.078302Z",
          "shell.execute_reply": "2023-03-05T14:02:45.092939Z"
        },
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Callbacks**\n",
        "<br>\n",
        "Callbacks : Helpful functions to help optimize model training  <br>\n",
        "Examples: stop model training after specfic time, stop training if no improve in accuracy and so on."
      ],
      "metadata": {
        "id": "_K-ryg0DGG6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCallback(keras.callbacks.Callback):\n",
        "    def __init__(self, model, patience, stop_patience, threshold, factor, batches, epochs, ask_epoch):\n",
        "        super(MyCallback, self).__init__()\n",
        "        self.model = model\n",
        "        self.patience = patience # specifies how many epochs without improvement before learning rate is adjusted\n",
        "        self.stop_patience = stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
        "        self.threshold = threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
        "        self.factor = factor # factor by which to reduce the learning rate\n",
        "        self.batches = batches # number of training batch to run per epoch\n",
        "        self.epochs = epochs\n",
        "        self.ask_epoch = ask_epoch\n",
        "        self.ask_epoch_initial = ask_epoch # save this value to restore if restarting training\n",
        "\n",
        "        # callback variables\n",
        "        self.count = 0 # how many times lr has been reduced without improvement\n",
        "        self.stop_count = 0\n",
        "        self.best_epoch = 1   # epoch with the lowest loss\n",
        "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initial learning rate and save it\n",
        "        self.highest_tracc = 0.0 # set highest training accuracy to 0 initially\n",
        "        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n",
        "        self.best_weights = self.model.get_weights() # set best weights to model's initial weights\n",
        "        self.initial_weights = self.model.get_weights()   # save initial weights if they have to get restored\n",
        "\n",
        "    # Define a function that will run when train begins\n",
        "    def on_train_begin(self, logs= None):\n",
        "        msg = 'Do you want model asks you to halt the training [y/n] ?'\n",
        "        print(msg)\n",
        "        ans = input('')\n",
        "        if ans in ['Y', 'y']:\n",
        "            self.ask_permission = 1\n",
        "        elif ans in ['N', 'n']:\n",
        "            self.ask_permission = 0\n",
        "\n",
        "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
        "        print(msg)\n",
        "        self.start_time = time.time()\n",
        "\n",
        "\n",
        "    def on_train_end(self, logs= None):\n",
        "        stop_time = time.time()\n",
        "        tr_duration = stop_time - self.start_time\n",
        "        hours = tr_duration // 3600\n",
        "        minutes = (tr_duration - (hours * 3600)) // 60\n",
        "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
        "\n",
        "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
        "        print(msg)\n",
        "\n",
        "        # set the weights of the model to the best weights\n",
        "        self.model.set_weights(self.best_weights)\n",
        "\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs= None):\n",
        "        # get batch accuracy and loss\n",
        "        acc = logs.get('accuracy') * 100\n",
        "        loss = logs.get('loss')\n",
        "\n",
        "        # prints over on the same line to show running batch count\n",
        "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
        "        print(msg, '\\r', end= '')\n",
        "\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs= None):\n",
        "        self.ep_start = time.time()\n",
        "\n",
        "\n",
        "    # Define method runs on the end of each epoch\n",
        "    def on_epoch_end(self, epoch, logs= None):\n",
        "        ep_end = time.time()\n",
        "        duration = ep_end - self.ep_start\n",
        "\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
        "        current_lr = lr\n",
        "        acc = logs.get('accuracy')  # get training accuracy\n",
        "        v_acc = logs.get('val_accuracy')  # get validation accuracy\n",
        "        loss = logs.get('loss')  # get training loss for this epoch\n",
        "        v_loss = logs.get('val_loss')  # get the validation loss for this epoch\n",
        "\n",
        "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
        "            monitor = 'accuracy'\n",
        "            if epoch == 0:\n",
        "                pimprov = 0.0\n",
        "            else:\n",
        "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc # define improvement of model progres\n",
        "\n",
        "            if acc > self.highest_tracc: # training accuracy improved in the epoch\n",
        "                self.highest_tracc = acc # set new highest training accuracy\n",
        "                self.best_weights = self.model.get_weights() # training accuracy improved so save the weights\n",
        "                self.count = 0 # set count to 0 since training accuracy improved\n",
        "                self.stop_count = 0 # set stop counter to 0\n",
        "                if v_loss < self.lowest_vloss:\n",
        "                    self.lowest_vloss = v_loss\n",
        "                self.best_epoch = epoch + 1  # set the value of best epoch for this epoch\n",
        "\n",
        "            else:\n",
        "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
        "                # if so adjust learning rate\n",
        "                if self.count >= self.patience - 1: # lr should be adjusted\n",
        "                    lr = lr * self.factor # adjust the learning by factor\n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
        "                    self.count = 0 # reset the count to 0\n",
        "                    self.stop_count = self.stop_count + 1 # count the number of consecutive lr adjustments\n",
        "                    self.count = 0 # reset counter\n",
        "                    if v_loss < self.lowest_vloss:\n",
        "                        self.lowest_vloss = v_loss\n",
        "                else:\n",
        "                    self.count = self.count + 1 # increment patience counter\n",
        "\n",
        "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
        "            monitor = 'val_loss'\n",
        "            if epoch == 0:\n",
        "                pimprov = 0.0\n",
        "\n",
        "            else:\n",
        "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
        "\n",
        "            if v_loss < self.lowest_vloss: # check if the validation loss improved\n",
        "                self.lowest_vloss = v_loss # replace lowest validation loss with new validation loss\n",
        "                self.best_weights = self.model.get_weights() # validation loss improved so save the weights\n",
        "                self.count = 0 # reset count since validation loss improved\n",
        "                self.stop_count = 0\n",
        "                self.best_epoch = epoch + 1 # set the value of the best epoch to this epoch\n",
        "\n",
        "            else: # validation loss did not improve\n",
        "                if self.count >= self.patience - 1: # need to adjust lr\n",
        "                    lr = lr * self.factor # adjust the learning rate\n",
        "                    self.stop_count = self.stop_count + 1 # increment stop counter because lr was adjusted\n",
        "                    self.count = 0 # reset counter\n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
        "\n",
        "                else:\n",
        "                    self.count = self.count + 1 # increment the patience counter\n",
        "\n",
        "                if acc > self.highest_tracc:\n",
        "                    self.highest_tracc = acc\n",
        "\n",
        "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
        "        print(msg)\n",
        "\n",
        "        if self.stop_count > self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
        "            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
        "            print(msg)\n",
        "            self.model.stop_training = True # stop training\n",
        "\n",
        "        else:\n",
        "            if self.ask_epoch != None and self.ask_permission != 0:\n",
        "                if epoch + 1 >= self.ask_epoch:\n",
        "                    msg = 'enter H to halt training or an integer for number of epochs to run then ask again'\n",
        "                    print(msg)\n",
        "\n",
        "                    ans = input('')\n",
        "                    if ans == 'H' or ans == 'h':\n",
        "                        msg = f'training has been halted at epoch {epoch + 1} due to user input'\n",
        "                        print(msg)\n",
        "                        self.model.stop_training = True # stop training\n",
        "\n",
        "                    else:\n",
        "                        try:\n",
        "                            ans = int(ans)\n",
        "                            self.ask_epoch += ans\n",
        "                            msg = f' training will continue until epoch {str(self.ask_epoch)}'\n",
        "                            print(msg)\n",
        "                            msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor', '% Improv', 'Duration')\n",
        "                            print(msg)\n",
        "\n",
        "                        except Exception:\n",
        "                            print('Invalid')"
      ],
      "metadata": {
        "id": "d5HiN8XDGG60",
        "execution": {
          "iopub.status.busy": "2023-03-05T14:02:45.099662Z",
          "iopub.execute_input": "2023-03-05T14:02:45.101163Z",
          "iopub.status.idle": "2023-03-05T14:02:45.14033Z",
          "shell.execute_reply.started": "2023-03-05T14:02:45.101089Z",
          "shell.execute_reply": "2023-03-05T14:02:45.138533Z"
        },
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Function to plot history of training**"
      ],
      "metadata": {
        "id": "2zwhoj3zGG61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training(hist):\n",
        "    '''\n",
        "    This function take training model and plot history of accuracy and losses with the best epoch in both of them.\n",
        "    '''\n",
        "\n",
        "    # Define needed variables\n",
        "    tr_acc = hist.history['accuracy']\n",
        "    tr_loss = hist.history['loss']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "    val_loss = hist.history['val_loss']\n",
        "\n",
        "    index_loss = np.argmin(val_loss)\n",
        "    val_lowest = val_loss[index_loss]\n",
        "    index_acc = np.argmax(val_acc)\n",
        "    acc_highest = val_acc[index_acc]\n",
        "\n",
        "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
        "\n",
        "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
        "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize= (20, 8))\n",
        "    plt.style.use('fivethirtyeight')\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
        "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
        "    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
        "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
        "    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "pU3eAW5jGG62",
        "execution": {
          "iopub.status.busy": "2023-03-05T14:02:45.142699Z",
          "iopub.execute_input": "2023-03-05T14:02:45.143415Z",
          "iopub.status.idle": "2023-03-05T14:02:45.162019Z",
          "shell.execute_reply.started": "2023-03-05T14:02:45.14332Z",
          "shell.execute_reply": "2023-03-05T14:02:45.159623Z"
        },
        "trusted": true
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Structure**"
      ],
      "metadata": {
        "id": "57eDFl3oGG65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Start Reading Dataset**"
      ],
      "metadata": {
        "id": "2GHNMVrhGG65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/kaggle/input/plantvillage-dataset/color'\n",
        "\n",
        "try:\n",
        "    # Get splitted data\n",
        "    train_df, valid_df, test_df = split_data(data_dir)\n",
        "\n",
        "    # Get Generators\n",
        "    batch_size = 40\n",
        "    train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, batch_size)\n",
        "\n",
        "except:\n",
        "    print('Invalid Input')"
      ],
      "metadata": {
        "id": "FWfxfQEVabWS",
        "outputId": "88cd2853-9f48-4552-eb0f-83fd6153d345",
        "execution": {
          "iopub.status.busy": "2023-03-05T14:02:45.184546Z",
          "iopub.execute_input": "2023-03-05T14:02:45.185588Z",
          "iopub.status.idle": "2023-03-05T14:03:43.106838Z",
          "shell.execute_reply.started": "2023-03-05T14:02:45.185528Z",
          "shell.execute_reply": "2023-03-05T14:03:43.105304Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid Input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Display Image Sample**"
      ],
      "metadata": {
        "id": "lQfLkPrLhHDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(train_gen)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-05T14:03:43.112685Z",
          "iopub.execute_input": "2023-03-05T14:03:43.113704Z",
          "iopub.status.idle": "2023-03-05T14:03:47.847214Z",
          "shell.execute_reply.started": "2023-03-05T14:03:43.113643Z",
          "shell.execute_reply": "2023-03-05T14:03:47.844948Z"
        },
        "trusted": true,
        "id": "VzuN0BGghHDt",
        "outputId": "38d59be6-5e8a-4a24-87d1-1177ada0a532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_gen' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-66a66b804b03>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_gen' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Generic Model Creation**"
      ],
      "metadata": {
        "id": "3wvOKjeRGG65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model Structure\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
        "\n",
        "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
        "# we will use efficientnetb3 from EfficientNet family.\n",
        "base_model = tf.keras.applications.efficientnet.EfficientNetB3(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
        "    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
        "                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
        "    Dropout(rate= 0.45, seed= 123),\n",
        "    Dense(class_count, activation= 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kDT4CV15abWT",
        "outputId": "12e46985-d43f-45b1-cc38-3528262ad399",
        "execution": {
          "iopub.status.busy": "2023-03-05T14:03:47.849425Z",
          "iopub.execute_input": "2023-03-05T14:03:47.850213Z",
          "iopub.status.idle": "2023-03-05T14:04:00.168146Z",
          "shell.execute_reply.started": "2023-03-05T14:03:47.850137Z",
          "shell.execute_reply": "2023-03-05T14:04:00.166597Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_gen' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-173ebc1a5236>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclass_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to define number of classes in dense layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_gen' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Set Callback Parameters**"
      ],
      "metadata": {
        "id": "TciwhdM1GG66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 40   # set batch size for training\n",
        "epochs = 40   # number of all epochs in training\n",
        "patience = 1   #number of epochs to wait to adjust lr if monitored value does not improve\n",
        "stop_patience = 3   # number of epochs to wait before stopping training if monitored value does not improve\n",
        "threshold = 0.9   # if train accuracy is < threshold adjust monitor accuracy, else monitor validation loss\n",
        "factor = 0.5   # factor to reduce lr by\n",
        "ask_epoch = 5   # number of epochs to run before asking if you want to halt training\n",
        "batches = int(np.ceil(len(train_gen.labels) / batch_size))    # number of training batch to run per epoch\n",
        "\n",
        "callbacks = [MyCallback(model= model, patience= patience, stop_patience= stop_patience, threshold= threshold,\n",
        "            factor= factor, batches= batches, epochs= epochs, ask_epoch= ask_epoch )]"
      ],
      "metadata": {
        "id": "7abvdv7mGG66",
        "execution": {
          "iopub.status.busy": "2023-03-05T14:04:00.170417Z",
          "iopub.execute_input": "2023-03-05T14:04:00.171406Z",
          "iopub.status.idle": "2023-03-05T14:04:00.558421Z",
          "shell.execute_reply.started": "2023-03-05T14:04:00.171343Z",
          "shell.execute_reply": "2023-03-05T14:04:00.556948Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Train model**"
      ],
      "metadata": {
        "id": "ap89fjdxGG67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x= train_gen, epochs= epochs, verbose= 0, callbacks= callbacks,\n",
        "                    validation_data= valid_gen, validation_steps= None, shuffle= False)"
      ],
      "metadata": {
        "id": "0Uk3BTERGG67",
        "execution": {
          "iopub.status.busy": "2023-03-05T14:04:00.560933Z",
          "iopub.execute_input": "2023-03-05T14:04:00.56151Z",
          "iopub.status.idle": "2023-03-05T14:53:47.253964Z",
          "shell.execute_reply.started": "2023-03-05T14:04:00.561446Z",
          "shell.execute_reply": "2023-03-05T14:53:47.252488Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Display model performance**"
      ],
      "metadata": {
        "id": "dNKq6ebOGG67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training(history)"
      ],
      "metadata": {
        "id": "L0Bj0Sp_GG68",
        "execution": {
          "iopub.status.busy": "2023-03-05T14:53:47.255731Z",
          "iopub.execute_input": "2023-03-05T14:53:47.256233Z",
          "iopub.status.idle": "2023-03-05T14:53:48.130042Z",
          "shell.execute_reply.started": "2023-03-05T14:53:47.25618Z",
          "shell.execute_reply": "2023-03-05T14:53:48.128368Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate model**"
      ],
      "metadata": {
        "id": "MySXhfAJGG68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts_length = len(test_df)\n",
        "test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
        "test_steps = ts_length // test_batch_size\n",
        "\n",
        "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
        "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
        "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
        "\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train Accuracy: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation Accuracy: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test Accuracy: \", test_score[1])"
      ],
      "metadata": {
        "id": "wSKDkyXXGG68",
        "execution": {
          "iopub.status.busy": "2023-03-05T14:53:48.132204Z",
          "iopub.execute_input": "2023-03-05T14:53:48.133104Z",
          "iopub.status.idle": "2023-03-05T15:00:04.414802Z",
          "shell.execute_reply.started": "2023-03-05T14:53:48.133047Z",
          "shell.execute_reply": "2023-03-05T15:00:04.413042Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get Predictions**"
      ],
      "metadata": {
        "id": "4l-DABtFGG68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict_generator(test_gen)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "GDFj7MZdGG69",
        "execution": {
          "iopub.status.busy": "2023-03-05T15:00:04.41984Z",
          "iopub.execute_input": "2023-03-05T15:00:04.420337Z",
          "iopub.status.idle": "2023-03-05T15:02:06.351133Z",
          "shell.execute_reply.started": "2023-03-05T15:00:04.420278Z",
          "shell.execute_reply": "2023-03-05T15:02:06.349335Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Confusion Matrics and Classification Report**"
      ],
      "metadata": {
        "id": "aJscUTF6GG69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_dict = test_gen.class_indices\n",
        "classes = list(g_dict.keys())\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(test_gen.classes, y_pred, target_names= classes))"
      ],
      "metadata": {
        "id": "tQR-UlD6GG69",
        "execution": {
          "iopub.status.busy": "2023-03-05T15:02:37.388014Z",
          "iopub.execute_input": "2023-03-05T15:02:37.388592Z",
          "iopub.status.idle": "2023-03-05T15:02:37.468401Z",
          "shell.execute_reply.started": "2023-03-05T15:02:37.388542Z",
          "shell.execute_reply": "2023-03-05T15:02:37.4667Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Save model**"
      ],
      "metadata": {
        "id": "SsIK5v0lGG69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model.input_names[0][:-6]\n",
        "subject = 'Plant Village Disease'\n",
        "acc = test_score[1] * 100\n",
        "save_path = ''\n",
        "\n",
        "# Save model\n",
        "save_id = str(f'{model_name}-{subject}-{\"%.2f\" %round(acc, 2)}.h5')\n",
        "model_save_loc = os.path.join(save_path, save_id)\n",
        "model.save(model_save_loc)\n",
        "print(f'model was saved as {model_save_loc}')\n",
        "\n",
        "# Save weights\n",
        "weight_save_id = str(f'{model_name}-{subject}-weights.h5')\n",
        "weights_save_loc = os.path.join(save_path, weight_save_id)\n",
        "model.save_weights(weights_save_loc)\n",
        "print(f'weights were saved as {weights_save_loc}')"
      ],
      "metadata": {
        "id": "oy5ShUciGG6-",
        "execution": {
          "iopub.status.busy": "2023-03-05T15:02:16.198119Z",
          "iopub.execute_input": "2023-03-05T15:02:16.199052Z",
          "iopub.status.idle": "2023-03-05T15:02:18.040588Z",
          "shell.execute_reply.started": "2023-03-05T15:02:16.198995Z",
          "shell.execute_reply": "2023-03-05T15:02:18.038851Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Generate CSV files containing classes indicies & image size**"
      ],
      "metadata": {
        "id": "q2fsiEtEGG6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_dict = train_gen.class_indices\n",
        "img_size = train_gen.image_shape\n",
        "height = []\n",
        "width = []\n",
        "for _ in range(len(class_dict)):\n",
        "    height.append(img_size[0])\n",
        "    width.append(img_size[1])\n",
        "\n",
        "Index_series = pd.Series(list(class_dict.values()), name= 'class_index')\n",
        "Class_series = pd.Series(list(class_dict.keys()), name= 'class')\n",
        "Height_series = pd.Series(height, name= 'height')\n",
        "Width_series = pd.Series(width, name= 'width')\n",
        "class_df = pd.concat([Index_series, Class_series, Height_series, Width_series], axis= 1)\n",
        "csv_name = f'{subject}-class_dict.csv'\n",
        "csv_save_loc = os.path.join(save_path, csv_name)\n",
        "class_df.to_csv(csv_save_loc, index= False)\n",
        "print(f'class csv file was saved as {csv_save_loc}')"
      ],
      "metadata": {
        "id": "UiHQzq8XGG6-",
        "execution": {
          "iopub.status.busy": "2023-03-05T15:02:18.042985Z",
          "iopub.execute_input": "2023-03-05T15:02:18.043565Z",
          "iopub.status.idle": "2023-03-05T15:02:18.06823Z",
          "shell.execute_reply.started": "2023-03-05T15:02:18.043512Z",
          "shell.execute_reply": "2023-03-05T15:02:18.064198Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Thank You..**\n",
        "If you find this notebook is good enough, please upvote it.."
      ],
      "metadata": {
        "id": "kXEhxrLshHDy"
      }
    }
  ]
}